{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "aebce404",
      "metadata": {
        "id": "aebce404"
      },
      "source": [
        "# 1. Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04dd78bc",
      "metadata": {
        "id": "04dd78bc"
      },
      "outputs": [],
      "source": [
        "# 1. Import required libraries\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        "    confusion_matrix\n",
        ")\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from skimage.feature import local_binary_pattern, hog, canny\n",
        "from skimage.exposure import equalize_hist\n",
        "from skimage.filters import gaussian\n",
        "from skimage import img_as_float\n",
        "from skimage.measure import label\n",
        "\n",
        "from torchvision.datasets import EMNIST\n",
        "from torchvision import transforms\n",
        "\n",
        "import torch\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32a31b3d",
      "metadata": {
        "id": "32a31b3d"
      },
      "source": [
        "# 2. Data Acquisition – Load EMNIST Letters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26032e55",
      "metadata": {
        "id": "26032e55",
        "outputId": "039fb2be-1fb2-4538-daaa-3773383f09fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 562M/562M [00:02<00:00, 272MB/s]\n"
          ]
        }
      ],
      "source": [
        "# 2. Data acquisition: load EMNIST Letters\n",
        "\n",
        "# Transform: just convert to tensor in [0,1], no augmentation\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Download EMNIST letters split\n",
        "train_dataset = EMNIST(\n",
        "    root=\"data\",\n",
        "    split=\"letters\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "test_dataset = EMNIST(\n",
        "    root=\"data\",\n",
        "    split=\"letters\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "len(train_dataset), len(test_dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee61bca0",
      "metadata": {
        "id": "ee61bca0"
      },
      "source": [
        "# 2.1 Convert Datasets to Numpy & (Optionally) Sample 10k–20k Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19c908dd",
      "metadata": {
        "id": "19c908dd"
      },
      "outputs": [],
      "source": [
        "# Helper: convert a torchvision dataset to numpy arrays\n",
        "def dataset_to_numpy(dataset, max_samples=None, seed=42):\n",
        "    rng = np.random.RandomState(seed)\n",
        "    n = len(dataset)\n",
        "    if max_samples is not None and max_samples < n:\n",
        "        indices = rng.choice(n, size=max_samples, replace=False)\n",
        "    else:\n",
        "        indices = np.arange(n)\n",
        "\n",
        "    images = []\n",
        "    labels = []\n",
        "    for idx in indices:\n",
        "        img_tensor, label = dataset[idx]   # img_tensor: [1, 28, 28]\n",
        "        img = img_tensor.squeeze(0).numpy()  # -> (28, 28)\n",
        "        images.append(img)\n",
        "        labels.append(label)\n",
        "\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# Take 20k samples from train data\n",
        "X_images, y_raw = dataset_to_numpy(train_dataset, max_samples=20000)\n",
        "\n",
        "X_images.shape, y_raw.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "859df600",
      "metadata": {
        "id": "859df600"
      },
      "source": [
        "# 2.2 Map Labels to A–Z and Show Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1a2fd54",
      "metadata": {
        "id": "a1a2fd54"
      },
      "outputs": [],
      "source": [
        "# EMNIST letters labels are 1-26; we map them to 0-25 internally\n",
        "y = y_raw - 1  # now in [0, 25]\n",
        "\n",
        "# Helper to map numeric label to actual letter (A-Z)\n",
        "def idx_to_char(idx):\n",
        "    return chr(ord('A') + idx)\n",
        "\n",
        "# Dataset size and category-wise count\n",
        "unique, counts = np.unique(y, return_counts=True)\n",
        "print(\"Total samples:\", len(y))\n",
        "print(\"Number of classes:\", len(unique))\n",
        "\n",
        "for cls, cnt in zip(unique, counts):\n",
        "    print(f\"Class {cls} ({idx_to_char(cls)}): {cnt}\")\n",
        "\n",
        "# Plot label distribution\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.bar([idx_to_char(i) for i in unique], counts)\n",
        "plt.xlabel(\"Letter Class\")\n",
        "plt.ylabel(\"Number of Samples\")\n",
        "plt.title(\"Label Distribution in EMNIST Letters (Sampled)\")\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4eb3b81b",
      "metadata": {
        "id": "4eb3b81b"
      },
      "source": [
        "# 3. Data Preparation – Train/Test Split"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a9bba27",
      "metadata": {
        "id": "5a9bba27"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6c9a41a",
      "metadata": {
        "id": "a6c9a41a"
      },
      "outputs": [],
      "source": [
        "# 3. Train-test split (stratified)\n",
        "\n",
        "X_train_imgs, X_test_imgs, y_train, y_test = train_test_split(\n",
        "    X_images,\n",
        "    y,\n",
        "    test_size=0.2,\n",
        "    stratify=y,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_train_imgs.shape, X_test_imgs.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b7ce150",
      "metadata": {
        "id": "3b7ce150"
      },
      "source": [
        "# 4. Preprocessing & Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5699e2b6",
      "metadata": {
        "id": "5699e2b6"
      },
      "outputs": [],
      "source": [
        "# 4. Feature engineering\n",
        "\n",
        "# Parameters for LBP\n",
        "LBP_RADIUS = 1\n",
        "LBP_POINTS = 8 * LBP_RADIUS\n",
        "LBP_METHOD = \"uniform\"\n",
        "\n",
        "def extract_features_single(img_28x28):\n",
        "    \"\"\"\n",
        "    img_28x28: numpy array (28x28), values in [0,1] or [0,255]\n",
        "    Returns: 1D feature vector (np.array)\n",
        "    \"\"\"\n",
        "    # Ensure float in [0,1]\n",
        "    img = img_as_float(img_28x28)\n",
        "\n",
        "    # --- Low-level preprocessing ---\n",
        "\n",
        "    # 1) Histogram equalization (improves contrast)\n",
        "    img_eq = equalize_hist(img)\n",
        "\n",
        "    # 2) Gaussian smoothing to reduce noise\n",
        "    img_smooth = gaussian(img_eq, sigma=1)\n",
        "\n",
        "    # --- Mid-level features ---\n",
        "\n",
        "    # A) Edge map using Canny\n",
        "    edges = canny(img_smooth, sigma=1)\n",
        "    edge_density = edges.mean()  # fraction of edge pixels\n",
        "\n",
        "    # B) Connected components on edge image\n",
        "    labeled = label(edges)\n",
        "    num_components = labeled.max()  # 0 if no edges\n",
        "\n",
        "    # C) LBP texture descriptor\n",
        "    lbp = local_binary_pattern(img_smooth, LBP_POINTS, LBP_RADIUS, method=LBP_METHOD)\n",
        "    n_bins = LBP_POINTS + 2  # uniform LBP\n",
        "    lbp_hist, _ = np.histogram(\n",
        "        lbp.ravel(),\n",
        "        bins=n_bins,\n",
        "        range=(0, n_bins),\n",
        "        density=True  # normalized histogram\n",
        "    )\n",
        "\n",
        "    # D) HOG descriptor (gradient-based shape descriptor)\n",
        "    hog_feat = hog(\n",
        "        img_smooth,\n",
        "        orientations=9,\n",
        "        pixels_per_cell=(7, 7),\n",
        "        cells_per_block=(2, 2),\n",
        "        block_norm=\"L2-Hys\",\n",
        "        transform_sqrt=True,\n",
        "        feature_vector=True\n",
        "    )\n",
        "\n",
        "    # Concatenate all features into a single vector\n",
        "    feature_vector = np.hstack([\n",
        "        lbp_hist,\n",
        "        hog_feat,\n",
        "        [edge_density, num_components]\n",
        "    ])\n",
        "\n",
        "    return feature_vector\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "017a7251",
      "metadata": {
        "id": "017a7251"
      },
      "source": [
        "# 4.2 Apply Feature Extraction to All Train & Test Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "dfc96faf",
      "metadata": {
        "id": "dfc96faf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c322071-58b3-46d4-dafc-9d7ac547b2fc"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/skimage/feature/texture.py:385: UserWarning: Applying `local_binary_pattern` to floating-point images may give unexpected results when small numerical differences between adjacent pixels are present. It is recommended to use this function with images of integer dtype.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((16000, 336), (4000, 336))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Apply feature extraction to all images\n",
        "def build_feature_matrix(images):\n",
        "    features = [extract_features_single(img) for img in images]\n",
        "    return np.vstack(features)\n",
        "\n",
        "X_train = build_feature_matrix(X_train_imgs)\n",
        "X_test = build_feature_matrix(X_test_imgs)\n",
        "\n",
        "X_train.shape, X_test.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1da6b00",
      "metadata": {
        "id": "f1da6b00"
      },
      "source": [
        "# 5. Model Building – kNN, SVM, RF, Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "44f8bb70",
      "metadata": {
        "id": "44f8bb70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31a2835b-58d1-452c-eeda-06f1295f145a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kNN': Pipeline(steps=[('standardscaler', StandardScaler()),\n",
              "                 ('kneighborsclassifier', KNeighborsClassifier())]),\n",
              " 'SVM (RBF)': Pipeline(steps=[('standardscaler', StandardScaler()), ('svc', SVC(C=10))]),\n",
              " 'RandomForest': RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=42),\n",
              " 'LogisticRegression': Pipeline(steps=[('standardscaler', StandardScaler()),\n",
              "                 ('logisticregression',\n",
              "                  LogisticRegression(max_iter=1000, multi_class='multinomial',\n",
              "                                     n_jobs=-1))])}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# 5. Model building\n",
        "\n",
        "models = {\n",
        "    \"kNN\": make_pipeline(\n",
        "        StandardScaler(),\n",
        "        KNeighborsClassifier(n_neighbors=5)\n",
        "    ),\n",
        "    \"SVM (RBF)\": make_pipeline(\n",
        "        StandardScaler(),\n",
        "        SVC(kernel=\"rbf\", C=10, gamma=\"scale\")\n",
        "    ),\n",
        "    \"RandomForest\": RandomForestClassifier(\n",
        "        n_estimators=200,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ),\n",
        "    \"LogisticRegression\": make_pipeline(\n",
        "        StandardScaler(),\n",
        "        LogisticRegression(\n",
        "            max_iter=1000,\n",
        "            multi_class=\"multinomial\",\n",
        "            n_jobs=-1\n",
        "        )\n",
        "    )\n",
        "}\n",
        "\n",
        "models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9028b4d9",
      "metadata": {
        "id": "9028b4d9"
      },
      "source": [
        "# 5.1 Train Models and Evaluate via Cross-Validation (on Train Set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "a876674c",
      "metadata": {
        "id": "a876674c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "outputId": "2ba88114-2171-49b2-c241-8da59f0827fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== kNN ===\n",
            "CV scores: [0.85451819 0.85467842 0.85036565]\n",
            "Mean CV accuracy: 0.853187416826975\n",
            "\n",
            "=== SVM (RBF) ===\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2813504353.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n=== {name} ===\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# 3-fold cross-validation on training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mcv_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CV scores:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mean CV accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 684\u001b[0;31m     cv_results = cross_validate(\n\u001b[0m\u001b[1;32m    685\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0;31m# independent, and that it is pickle-able.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m     results = parallel(\n\u001b[0m\u001b[1;32m    412\u001b[0m         delayed(_fit_and_score)(\n\u001b[1;32m    413\u001b[0m             \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2070\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2072\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1682\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1798\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTASK_PENDING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m                 ):\n\u001b[0;32m-> 1800\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1801\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "cv_results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    # 3-fold cross-validation on training data\n",
        "    cv_scores = cross_val_score(model, X_train, y_train, cv=3, n_jobs=-1)\n",
        "    print(\"CV scores:\", cv_scores)\n",
        "    print(\"Mean CV accuracy:\", cv_scores.mean())\n",
        "    cv_results[name] = cv_scores.mean()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7887c883",
      "metadata": {
        "id": "7887c883"
      },
      "source": [
        "# 5.2 Fit Models on Full Training Set & Evaluate on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94b9d5e3",
      "metadata": {
        "id": "94b9d5e3"
      },
      "outputs": [],
      "source": [
        "test_accuracies = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTraining {name} on full training data...\")\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    test_accuracies[name] = acc\n",
        "    print(f\"Test accuracy: {acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97e89e05",
      "metadata": {
        "id": "97e89e05"
      },
      "source": [
        "# 5.3 Compare Model Accuracies in a Bar Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "411190a4",
      "metadata": {
        "id": "411190a4"
      },
      "outputs": [],
      "source": [
        "# Combine CV and test accuracy into a simple bar plot\n",
        "model_names = list(models.keys())\n",
        "cv_vals = [cv_results[m] for m in model_names]\n",
        "test_vals = [test_accuracies[m] for m in model_names]\n",
        "\n",
        "x = np.arange(len(model_names))\n",
        "width = 0.35\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.bar(x - width/2, cv_vals, width, label=\"CV Accuracy\")\n",
        "plt.bar(x + width/2, test_vals, width, label=\"Test Accuracy\")\n",
        "plt.xticks(x, model_names, rotation=15)\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Model Accuracies on EMNIST Letters (Handcrafted Features)\")\n",
        "plt.legend()\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df70c583",
      "metadata": {
        "id": "df70c583"
      },
      "source": [
        "# 6. Detailed Metrics for the Best Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64507b7b",
      "metadata": {
        "id": "64507b7b"
      },
      "outputs": [],
      "source": [
        "# Find best model by test accuracy\n",
        "best_model_name = max(test_accuracies, key=test_accuracies.get)\n",
        "best_model = models[best_model_name]\n",
        "\n",
        "print(f\"Best model: {best_model_name}, Test accuracy = {test_accuracies[best_model_name]:.4f}\")\n",
        "\n",
        "# Predict on test set\n",
        "y_pred_best = best_model.predict(X_test)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(\n",
        "    y_test,\n",
        "    y_pred_best,\n",
        "    target_names=[idx_to_char(i) for i in range(26)]\n",
        "))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf488861",
      "metadata": {
        "id": "bf488861"
      },
      "source": [
        "# 6.1 Confusion Matrix Heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9f31d4c",
      "metadata": {
        "id": "a9f31d4c"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_test, y_pred_best)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.imshow(cm, interpolation=\"nearest\")\n",
        "plt.title(f\"Confusion Matrix - {best_model_name}\")\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(26)\n",
        "plt.xticks(tick_marks, [idx_to_char(i) for i in range(26)], rotation=90)\n",
        "plt.yticks(tick_marks, [idx_to_char(i) for i in range(26)])\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3a12ec6",
      "metadata": {
        "id": "b3a12ec6"
      },
      "source": [
        "# 7. Model Inference & Evaluation – Show 5 Random Test Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31123e28",
      "metadata": {
        "id": "31123e28"
      },
      "outputs": [],
      "source": [
        "# 7. Visual evaluation on 5 random test images\n",
        "\n",
        "def show_random_predictions(num_samples=5):\n",
        "    \"\"\"\n",
        "    Pick 'num_samples' random test images and display Actual vs Predicted labels.\n",
        "    Each call gives different images.\n",
        "    \"\"\"\n",
        "    # Randomly pick indices\n",
        "    indices = np.random.choice(len(X_test_imgs), size=num_samples, replace=False)\n",
        "\n",
        "    plt.figure(figsize=(12, 3))\n",
        "\n",
        "    for i, idx in enumerate(indices):\n",
        "        img = X_test_imgs[idx]\n",
        "        true_label = y_test[idx]\n",
        "\n",
        "        # Extract features & predict\n",
        "        feat = extract_features_single(img).reshape(1, -1)\n",
        "        pred_label = best_model.predict(feat)[0]\n",
        "\n",
        "        # Plot image\n",
        "        plt.subplot(1, num_samples, i + 1)\n",
        "        plt.imshow(img, cmap=\"gray\")\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(f\"True: {idx_to_char(true_label)}\\nPred: {idx_to_char(pred_label)}\")\n",
        "\n",
        "    plt.suptitle(f\"Random Test Predictions ({best_model_name})\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "show_random_predictions()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c90ad954",
      "metadata": {
        "id": "c90ad954"
      },
      "source": [
        "# 8. Validation of Actual Test – Your Own Handwritten Letter"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc8bb375",
      "metadata": {
        "id": "dc8bb375"
      },
      "source": [
        "## 8.1 Function to Process an External Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89704535",
      "metadata": {
        "id": "89704535"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "def preprocess_external_image(path, invert=True):\n",
        "    \"\"\"\n",
        "    Load an external handwritten letter image, convert it to the same format\n",
        "    as EMNIST, and extract features.\n",
        "    \"\"\"\n",
        "    # Load image with PIL\n",
        "    img = Image.open(path).convert(\"L\")  # convert to grayscale\n",
        "\n",
        "    # Resize to 28x28 (EMNIST size)\n",
        "    img = img.resize((28, 28))\n",
        "\n",
        "    # Convert to numpy array\n",
        "    img_np = np.array(img).astype(\"float32\")\n",
        "\n",
        "    # Depending on background (white vs black), we may need to invert\n",
        "    if invert:\n",
        "        img_np = 255.0 - img_np\n",
        "\n",
        "    # Normalize to [0,1]\n",
        "    img_np = img_np / 255.0\n",
        "\n",
        "    return img_np\n",
        "\n",
        "def predict_external_image(path, invert=True):\n",
        "    img_28x28 = preprocess_external_image(path, invert=invert)\n",
        "    features = extract_features_single(img_28x28).reshape(1, -1)\n",
        "    pred_label = best_model.predict(features)[0]\n",
        "    pred_char = idx_to_char(pred_label)\n",
        "\n",
        "    plt.figure(figsize=(3, 3))\n",
        "    plt.imshow(img_28x28, cmap=\"gray\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Predicted: {pred_char}\")\n",
        "    plt.show()\n",
        "\n",
        "    return pred_char\n",
        "\n",
        "# Example usage (once you have the image file in your environment):\n",
        "# my_pred = predict_external_image(\"my_letter.png\", invert=True)\n",
        "# print(\"Model prediction for my handwritten sample:\", my_pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "097bda0c",
      "metadata": {
        "id": "097bda0c"
      },
      "outputs": [],
      "source": [
        "# Example usage (once you have the image file in your environment):\n",
        "my_pred = predict_external_image(\"a.png\", invert=True)\n",
        "print(\"Model prediction for my handwritten sample:\", my_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9613ddf",
      "metadata": {
        "id": "b9613ddf"
      },
      "outputs": [],
      "source": [
        "my_pred = predict_external_image(\"b.png\", invert=True)\n",
        "print(\"Model prediction for my handwritten sample:\", my_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e3d97ae",
      "metadata": {
        "id": "9e3d97ae"
      },
      "outputs": [],
      "source": [
        "my_pred = predict_external_image(\"ma.png\", invert=True)\n",
        "print(\"Model prediction for my handwritten sample:\", my_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af0d83ab",
      "metadata": {
        "id": "af0d83ab"
      },
      "outputs": [],
      "source": [
        "my_pred = predict_external_image(\"k.png\", invert=True)\n",
        "print(\"Model prediction for my handwritten sample:\", my_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "524c06b6",
      "metadata": {
        "id": "524c06b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "53b095bd-e834-457b-c575-855fdbd05c37"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'wpng.png'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-772709290.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_external_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wpng.png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model prediction for my handwritten sample:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1587877885.py\u001b[0m in \u001b[0;36mpredict_external_image\u001b[0;34m(path, invert)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_external_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mimg_28x28\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_external_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_28x28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mpred_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1587877885.py\u001b[0m in \u001b[0;36mpreprocess_external_image\u001b[0;34m(path, invert)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \"\"\"\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Load image with PIL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"L\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# convert to grayscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Resize to 28x28 (EMNIST size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3511\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3512\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3513\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3514\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3515\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'wpng.png'"
          ]
        }
      ],
      "source": [
        "my_pred = predict_external_image(\"wpng.png\", invert=True)\n",
        "print(\"Model prediction for my handwritten sample:\", my_pred)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "AI1103p11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}