{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aebce404",
   "metadata": {
    "id": "aebce404"
   },
   "source": [
    "# 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dd78bc",
   "metadata": {
    "id": "04dd78bc"
   },
   "outputs": [],
   "source": [
    "# 1. Import required libraries\n\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n\n",
    "from pathlib import Path\n",
    "import random\n\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n\n",
    "from skimage.feature import local_binary_pattern, hog, canny\n",
    "from skimage.exposure import equalize_hist\n",
    "from skimage.filters import gaussian\n",
    "from skimage import img_as_float\n",
    "from skimage.measure import label\n\n",
    "from torchvision.datasets import EMNIST\n",
    "from torchvision import transforms\n\n",
    "import torch\n\n",
    "# Make experiments reproducible\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a31b3d",
   "metadata": {
    "id": "32a31b3d"
   },
   "source": [
    "# 2. Data Acquisition \u2013 Load EMNIST Letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26032e55",
   "metadata": {
    "id": "26032e55",
    "outputId": "039fb2be-1fb2-4538-daaa-3773383f09fc",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Data acquisition: load EMNIST Letters\n",
    "\n",
    "# Transform: just convert to tensor in [0,1], no augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Download EMNIST letters split\n",
    "train_dataset = EMNIST(\n",
    "    root=\"data\",\n",
    "    split=\"letters\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = EMNIST(\n",
    "    root=\"data\",\n",
    "    split=\"letters\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "len(train_dataset), len(test_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee61bca0",
   "metadata": {
    "id": "ee61bca0"
   },
   "source": [
    "# 2.1 Convert Datasets to Numpy & (Optionally) Sample 10k\u201320k Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c908dd",
   "metadata": {
    "id": "19c908dd"
   },
   "outputs": [],
   "source": [
    "# Helper: convert a torchvision dataset to numpy arrays\n",
    "def dataset_to_numpy(dataset, max_samples=None, seed=42):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    n = len(dataset)\n",
    "    if max_samples is not None and max_samples < n:\n",
    "        indices = rng.choice(n, size=max_samples, replace=False)\n",
    "    else:\n",
    "        indices = np.arange(n)\n",
    "\n",
    "    images = []\n",
    "    labels = []\n",
    "    for idx in indices:\n",
    "        img_tensor, label = dataset[idx]   # img_tensor: [1, 28, 28]\n",
    "        img = img_tensor.squeeze(0).numpy()  # -> (28, 28)\n",
    "        images.append(img)\n",
    "        labels.append(label)\n",
    "\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Take 20k samples from train data\n",
    "X_images, y_raw = dataset_to_numpy(train_dataset, max_samples=20000)\n",
    "\n",
    "X_images.shape, y_raw.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859df600",
   "metadata": {
    "id": "859df600"
   },
   "source": [
    "# 2.2 Map Labels to A\u2013Z and Show Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a2fd54",
   "metadata": {
    "id": "a1a2fd54"
   },
   "outputs": [],
   "source": [
    "# EMNIST letters labels are 1-26; we map them to 0-25 internally\n",
    "y = y_raw - 1  # now in [0, 25]\n\n",
    "# Helper to map numeric label to actual letter (A-Z)\n",
    "def idx_to_char(idx):\n",
    "    return chr(ord('A') + idx)\n\n",
    "# Dataset size and category-wise count\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "print(\"Total samples:\", len(y))\n",
    "print(\"Number of classes:\", len(unique))\n\n",
    "distribution_df = pd.DataFrame({\"label\": [idx_to_char(i) for i in unique], \"count\": counts})\n",
    "display(distribution_df.head())\n\n",
    "for cls, cnt in zip(unique, counts):\n",
    "    print(f\"Class {cls} ({idx_to_char(cls)}): {cnt}\")\n\n",
    "# Plot label distribution\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar([idx_to_char(i) for i in unique], counts)\n",
    "plt.xlabel(\"Letter Class\")\n",
    "plt.ylabel(\"Number of Samples\")\n",
    "plt.title(\"Label Distribution in EMNIST Letters (Sampled)\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(counts, labels=[idx_to_char(i) for i in unique], autopct='%1.1f%%', textprops={'fontsize':6})\n",
    "plt.title(\"Label distribution (pie)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb3b81b",
   "metadata": {
    "id": "4eb3b81b"
   },
   "source": [
    "# 3. Data Preparation \u2013 Train/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9bba27",
   "metadata": {
    "id": "5a9bba27"
   },
   "source": [
    "We perform an 80/20 stratified split on the sampled images so every letter keeps its proportion in both train and test sets. This matches the rubric requirement for a balanced evaluation pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c9a41a",
   "metadata": {
    "id": "a6c9a41a"
   },
   "outputs": [],
   "source": [
    "# 3. Train-test split (stratified)\n\n",
    "X_train_imgs, X_test_imgs, y_train, y_test = train_test_split(\n",
    "    X_images,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=SEED\n",
    ")\n\n",
    "print(f'Train images: {len(X_train_imgs)} | Test images: {len(X_test_imgs)}')\n",
    "print(f'Class distribution preserved (train): {np.bincount(y_train)}')\n",
    "print(f'Class distribution preserved (test): {np.bincount(y_test)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ce150",
   "metadata": {
    "id": "3b7ce150"
   },
   "source": [
    "# 4. Preprocessing & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5699e2b6",
   "metadata": {
    "id": "5699e2b6"
   },
   "outputs": [],
   "source": [
    "# 4. Feature engineering\n",
    "\n",
    "# Parameters for LBP\n",
    "LBP_RADIUS = 1\n",
    "LBP_POINTS = 8 * LBP_RADIUS\n",
    "LBP_METHOD = \"uniform\"\n",
    "\n",
    "def extract_features_single(img_28x28):\n",
    "    \"\"\"\n",
    "    img_28x28: numpy array (28x28), values in [0,1] or [0,255]\n",
    "    Returns: 1D feature vector (np.array)\n",
    "    \"\"\"\n",
    "    # Ensure float in [0,1]\n",
    "    img = img_as_float(img_28x28)\n",
    "\n",
    "    # --- Low-level preprocessing ---\n",
    "\n",
    "    # 1) Histogram equalization (improves contrast)\n",
    "    img_eq = equalize_hist(img)\n",
    "\n",
    "    # 2) Gaussian smoothing to reduce noise\n",
    "    img_smooth = gaussian(img_eq, sigma=1)\n",
    "\n",
    "    # --- Mid-level features ---\n",
    "\n",
    "    # A) Edge map using Canny\n",
    "    edges = canny(img_smooth, sigma=1)\n",
    "    edge_density = edges.mean()  # fraction of edge pixels\n",
    "\n",
    "    # B) Connected components on edge image\n",
    "    labeled = label(edges)\n",
    "    num_components = labeled.max()  # 0 if no edges\n",
    "\n",
    "    # C) LBP texture descriptor\n",
    "    # Convert to uint8 before LBP to avoid floating-point sensitivity warnings\n",
    "    lbp_input = np.round(img_smooth * 255).astype(np.uint8)\n",
    "    lbp = local_binary_pattern(lbp_input, LBP_POINTS, LBP_RADIUS, method=LBP_METHOD)\n",
    "    n_bins = LBP_POINTS + 2  # uniform LBP\n",
    "    lbp_hist, _ = np.histogram(\n",
    "        lbp.ravel(),\n",
    "        bins=n_bins,\n",
    "        range=(0, n_bins),\n",
    "        density=True  # normalized histogram\n",
    "    )\n",
    "\n",
    "    # D) HOG descriptor (gradient-based shape descriptor)\n",
    "    hog_feat = hog(\n",
    "        img_smooth,\n",
    "        orientations=9,\n",
    "        pixels_per_cell=(7, 7),\n",
    "        cells_per_block=(2, 2),\n",
    "        block_norm=\"L2-Hys\",\n",
    "        transform_sqrt=True,\n",
    "        feature_vector=True\n",
    "    )\n",
    "\n",
    "    # Concatenate all features into a single vector\n",
    "    feature_vector = np.hstack([\n",
    "        lbp_hist,\n",
    "        hog_feat,\n",
    "        [edge_density, num_components]\n",
    "    ])\n",
    "\n",
    "    return feature_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017a7251",
   "metadata": {
    "id": "017a7251"
   },
   "source": [
    "# 4.2 Apply Feature Extraction to All Train & Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc96faf",
   "metadata": {
    "id": "dfc96faf",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9c322071-58b3-46d4-dafc-9d7ac547b2fc"
   },
   "outputs": [],
   "source": [
    "# Apply feature extraction to all images\n",
    "def build_feature_matrix(images):\n",
    "    features = [extract_features_single(img) for img in images]\n",
    "    return np.vstack(features)\n\n",
    "print('Extracting features for training set...')\n",
    "X_train = build_feature_matrix(X_train_imgs)\n",
    "print('Extracting features for test set...')\n",
    "X_test = build_feature_matrix(X_test_imgs)\n\n",
    "X_train.shape, X_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1da6b00",
   "metadata": {
    "id": "f1da6b00"
   },
   "source": [
    "# 5. Model Building \u2013 kNN, SVM, RF, Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f8bb70",
   "metadata": {
    "id": "44f8bb70",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "31a2835b-58d1-452c-eeda-06f1295f145a"
   },
   "outputs": [],
   "source": [
    "# 5. Model building\n",
    "\n",
    "models = {\n",
    "    \"kNN\": make_pipeline(\n",
    "        StandardScaler(),\n",
    "        KNeighborsClassifier(n_neighbors=5)\n",
    "    ),\n",
    "    \"SVM (RBF)\": make_pipeline(\n",
    "        StandardScaler(),\n",
    "        SVC(kernel=\"rbf\", C=10, gamma=\"scale\")\n",
    "    ),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    \"LogisticRegression\": make_pipeline(\n",
    "        StandardScaler(),\n",
    "        LogisticRegression(\n",
    "            max_iter=1000,\n",
    "            multi_class=\"multinomial\",\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    )\n",
    "}\n",
    "\n",
    "models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9028b4d9",
   "metadata": {
    "id": "9028b4d9"
   },
   "source": [
    "# 5.1 Train Models and Evaluate via Cross-Validation (on Train Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a876674c",
   "metadata": {
    "id": "a876674c",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "outputId": "2ba88114-2171-49b2-c241-8da59f0827fb"
   },
   "outputs": [],
   "source": [
    "cv_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    # 3-fold cross-validation on training data\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=3, n_jobs=-1)\n",
    "    print(\"CV scores:\", cv_scores)\n",
    "    print(\"Mean CV accuracy:\", cv_scores.mean())\n",
    "    cv_results[name] = cv_scores.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7887c883",
   "metadata": {
    "id": "7887c883"
   },
   "source": [
    "# 5.2 Fit Models on Full Training Set & Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b9d5e3",
   "metadata": {
    "id": "94b9d5e3"
   },
   "outputs": [],
   "source": [
    "test_accuracies = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name} on full training data...\")\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    test_accuracies[name] = acc\n",
    "    print(f\"Test accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e89e05",
   "metadata": {
    "id": "97e89e05"
   },
   "source": [
    "# 5.3 Compare Model Accuracies in a Bar Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411190a4",
   "metadata": {
    "id": "411190a4"
   },
   "outputs": [],
   "source": [
    "# Combine CV and test accuracy into a simple bar plot\n",
    "model_names = list(models.keys())\n",
    "cv_vals = [cv_results[m] for m in model_names]\n",
    "test_vals = [test_accuracies[m] for m in model_names]\n\n",
    "results_df = pd.DataFrame({\"Model\": model_names, \"CV Accuracy\": cv_vals, \"Test Accuracy\": test_vals})\n",
    "display(results_df)\n\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.35\n\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(x - width/2, cv_vals, width, label=\"CV Accuracy\")\n",
    "plt.bar(x + width/2, test_vals, width, label=\"Test Accuracy\")\n",
    "plt.xticks(x, model_names, rotation=15)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Model Accuracies on EMNIST Letters (Handcrafted Features)\")\n",
    "plt.legend()\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df70c583",
   "metadata": {
    "id": "df70c583"
   },
   "source": [
    "# 6. Detailed Metrics for the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64507b7b",
   "metadata": {
    "id": "64507b7b"
   },
   "outputs": [],
   "source": [
    "# Find best model by test accuracy\n",
    "best_model_name = max(test_accuracies, key=test_accuracies.get)\n",
    "best_model = models[best_model_name]\n\n",
    "print(f\"Best model: {best_model_name}, Test accuracy = {test_accuracies[best_model_name]:.4f}\")\n\n",
    "# Predict on test set\n",
    "y_pred_best = best_model.predict(X_test)\n\n",
    "acc = accuracy_score(y_test, y_pred_best)\n",
    "f1 = f1_score(y_test, y_pred_best, average='macro')\n",
    "print(f'Hold-out accuracy: {acc:.4f}')\n",
    "print(f'Macro F1-score: {f1:.4f}')\n\n",
    "print(\"\nClassification Report:\")\n",
    "print(classification_report(\n",
    "    y_test,\n",
    "    y_pred_best,\n",
    "    target_names=[idx_to_char(i) for i in range(26)]\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf488861",
   "metadata": {
    "id": "bf488861"
   },
   "source": [
    "# 6.1 Confusion Matrix Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f31d4c",
   "metadata": {
    "id": "a9f31d4c"
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(cm, interpolation=\"nearest\")\n",
    "plt.title(f\"Confusion Matrix - {best_model_name}\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(26)\n",
    "plt.xticks(tick_marks, [idx_to_char(i) for i in range(26)], rotation=90)\n",
    "plt.yticks(tick_marks, [idx_to_char(i) for i in range(26)])\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why these handcrafted features?\n",
    "- **LBP histograms** capture local texture strokes and work well for thin handwriting lines.\n",
    "- **HOG descriptors** summarize gradient orientation structure, giving the model shape cues similar to classical OCR.\n",
    "- **Edge density + connected components** quantify how dense and fragmented the strokes are, which helps differentiate letters such as 'E' vs. 'F'.\n",
    "\nTogether they provide complementary texture and shape information, which is why the best-performing model tends to favor them over any single descriptor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a12ec6",
   "metadata": {
    "id": "b3a12ec6"
   },
   "source": [
    "# 7. Model Inference & Evaluation \u2013 Show 5 Random Test Images\n\n",
    "This section visualizes five random test samples with predicted vs. actual labels so we can qualitatively inspect mistakes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31123e28",
   "metadata": {
    "id": "31123e28"
   },
   "outputs": [],
   "source": [
    "# 7. Visual evaluation on 5 random test images\n",
    "\n",
    "def show_random_predictions(num_samples=5):\n",
    "    \"\"\"\n",
    "    Pick 'num_samples' random test images and display Actual vs Predicted labels.\n",
    "    Each call gives different images.\n",
    "    \"\"\"\n",
    "    # Randomly pick indices\n",
    "    indices = np.random.choice(len(X_test_imgs), size=num_samples, replace=False)\n",
    "\n",
    "    plt.figure(figsize=(12, 3))\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        img = X_test_imgs[idx]\n",
    "        true_label = y_test[idx]\n",
    "\n",
    "        # Extract features & predict\n",
    "        feat = extract_features_single(img).reshape(1, -1)\n",
    "        pred_label = best_model.predict(feat)[0]\n",
    "\n",
    "        # Plot image\n",
    "        plt.subplot(1, num_samples, i + 1)\n",
    "        plt.imshow(img, cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"True: {idx_to_char(true_label)}\\nPred: {idx_to_char(pred_label)}\")\n",
    "\n",
    "    plt.suptitle(f\"Random Test Predictions ({best_model_name})\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show_random_predictions()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90ad954",
   "metadata": {
    "id": "c90ad954"
   },
   "source": [
    "# 8. Validation of Actual Test \u2013 Your Own Handwritten Letter\n\n",
    "Upload or draw one handwritten letter image (white background preferred), then run the helper cell below to preprocess and evaluate it. Document whether the prediction matches your intent and discuss lighting/noise if it fails.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8bb375",
   "metadata": {
    "id": "dc8bb375"
   },
   "source": [
    "## 8.1 Function to Process an External Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89704535",
   "metadata": {
    "id": "89704535"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n\n",
    "def preprocess_external_image(path, invert=True):\n",
    "    \"\"\"\n",
    "    Load an external handwritten letter image, convert it to the same format\n",
    "    as EMNIST, and extract features.\n",
    "    \"\"\"\n",
    "    # Load image with PIL\n",
    "    img = Image.open(path).convert(\"L\")  # convert to grayscale\n\n",
    "    # Resize to 28x28 (EMNIST size)\n",
    "    img = img.resize((28, 28))\n\n",
    "    # Convert to numpy array\n",
    "    img_np = np.array(img).astype(\"float32\")\n\n",
    "    # Depending on background (white vs black), we may need to invert\n",
    "    if invert:\n",
    "        img_np = 255.0 - img_np\n\n",
    "    # Normalize to [0,1]\n",
    "    img_np = img_np / 255.0\n\n",
    "    return img_np\n\n",
    "def predict_external_image(path, invert=True):\n",
    "    img_28x28 = preprocess_external_image(path, invert=invert)\n",
    "    features = extract_features_single(img_28x28).reshape(1, -1)\n",
    "    pred_label = best_model.predict(features)[0]\n",
    "    pred_char = idx_to_char(pred_label)\n\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    plt.imshow(img_28x28, cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Predicted: {pred_char}\")\n",
    "    plt.show()\n\n",
    "    return pred_char\n\n",
    "# To evaluate your own handwritten letter, place it in the repo root (e.g., 'my_letter.png')\n",
    "# and run the next cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097bda0c",
   "metadata": {
    "id": "097bda0c"
   },
   "outputs": [],
   "source": [
    "# Evaluate one or more handwritten samples if they exist locally\n",
    "candidate_files = [\n",
    "    Path(\"my_letter.png\"),\n",
    "    Path(\"a.png\"),\n",
    "    Path(\"b.png\"),\n",
    "    Path(\"ma.png\"),\n",
    "    Path(\"k.png\"),\n",
    "    Path(\"wpng.png\"),\n",
    "]\n\n",
    "for path in candidate_files:\n",
    "    if path.exists():\n",
    "        pred = predict_external_image(path, invert=True)\n",
    "        print(f\"Model prediction for {path.name}: {pred}\")\n",
    "    else:\n",
    "        print(f\"Sample {path} not found; add your handwritten letter to test real-world performance.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "b9613ddf",
   "metadata": {
    "id": "b9613ddf"
   },
   "outputs": [],
   "source": [
    "(Reserved for additional handwritten samples. Reuse the evaluation cell above with your own file names.)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI1103p11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}